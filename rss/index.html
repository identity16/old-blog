<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Identity16</title><description>Unidentified Written Posts..</description><link>http://identity16.github.io/</link><image><url>http://identity16.github.io/favicon.png</url><title>Identity16</title><link>http://identity16.github.io/</link></image><generator>Ghost 2.9</generator><lastBuildDate>Mon, 14 Jan 2019 15:21:16 GMT</lastBuildDate><atom:link href="http://identity16.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>5.3 The Basics of Caches</title><description>&lt;h1 id="thebasicsofcaches"&gt;The Basics of Caches&lt;/h1&gt;
&lt;h2 id="questions"&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;데이터가 존재하는지 어떻게 확인?&lt;/li&gt;
&lt;li&gt;우리가 어디를 보고 어디에 넣어?
&lt;ol&gt;
&lt;li&gt;Directed-mapped&lt;/li&gt;
&lt;li&gt;Fully associative&lt;/li&gt;
&lt;li&gt;M-way Set associative&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="directivemappedcache"&gt;Directive Mapped Cache&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;위치가 주소에 의해 결정됨&lt;/li&gt;
&lt;li&gt;나머지 연산을 이용해 캐시 주소 결정
&lt;ul&gt;
&lt;li&gt;(주소) % (캐시 블럭 수)&lt;/li&gt;
&lt;li&gt;캐시 블럭 수는 2의 거듭제곱이다.&lt;/li&gt;
&lt;li&gt;주소 하위 비트 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tagsandvalidbits"&gt;Tags and Valid Bits&lt;/h2&gt;</description><link>http://identity16.github.io/5-3-the-basics-of-caches/</link><guid isPermaLink="false">5c35fbb82d91b6c5113154e2</guid><category>computer-architecture</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Fri, 21 Dec 2018 01:45:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/anatomy-1751201_1280.png" medium="image"/><content:encoded>&lt;h1 id="thebasicsofcaches"&gt;The Basics of Caches&lt;/h1&gt;
&lt;h2 id="questions"&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;데이터가 존재하는지 어떻게 확인?&lt;/li&gt;
&lt;li&gt;우리가 어디를 보고 어디에 넣어?
&lt;ol&gt;
&lt;li&gt;Directed-mapped&lt;/li&gt;
&lt;li&gt;Fully associative&lt;/li&gt;
&lt;li&gt;M-way Set associative&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="directivemappedcache"&gt;Directive Mapped Cache&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;위치가 주소에 의해 결정됨&lt;/li&gt;
&lt;li&gt;나머지 연산을 이용해 캐시 주소 결정
&lt;ul&gt;
&lt;li&gt;(주소) % (캐시 블럭 수)&lt;/li&gt;
&lt;li&gt;캐시 블럭 수는 2의 거듭제곱이다.&lt;/li&gt;
&lt;li&gt;주소 하위 비트 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tagsandvalidbits"&gt;Tags and Valid Bits&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;어떤 블럭이 캐시에 있는지 알 수 있을까?
&lt;ul&gt;
&lt;li&gt;블럭 주소도 저장해놓는다.&lt;/li&gt;
&lt;li&gt;상위 비트를 저장 =&amp;gt; 태그&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;데이터가 아예 없다면?
&lt;ul&gt;
&lt;li&gt;Valid bit로 표시&lt;/li&gt;
&lt;li&gt;1 = 있, 0 = 없 (초기값 0)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>5.2 Memory Technologies</title><description>&lt;h1 id="memorytechnologies"&gt;Memory Technologies&lt;/h1&gt;
&lt;p&gt;(시간이 급한 관계로 몇 가지만 추려서 정리했습니다.)&lt;/p&gt;
&lt;h2 id="memorytechnology"&gt;Memory Technology&lt;/h2&gt;
&lt;p&gt;  대표적인 메모리의 종류는 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static RAM(SRAM)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic RAM(DRAM)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Magnetic Disk&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;  SRAM -&amp;gt; DRAM -&amp;gt; Magnetic Disk로 갈수록 접근 속도가 100배 빨라지고 GB 당 가격이 100 배 낮아진다.&lt;/p&gt;
&lt;p&gt;빠른 메모리는 비싸기 때문에 이들을 적절히 혼용하여&lt;/p&gt;</description><link>http://identity16.github.io/5-2-memory-technologies/</link><guid isPermaLink="false">5c35fbe52d91b6c5113154e7</guid><category>computer-architecture</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Thu, 20 Dec 2018 17:39:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/hard-disk-775847_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="memorytechnologies"&gt;Memory Technologies&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/hard-disk-775847_1280.jpg" alt="5.2 Memory Technologies"&gt;&lt;p&gt;(시간이 급한 관계로 몇 가지만 추려서 정리했습니다.)&lt;/p&gt;
&lt;h2 id="memorytechnology"&gt;Memory Technology&lt;/h2&gt;
&lt;p&gt;  대표적인 메모리의 종류는 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static RAM(SRAM)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic RAM(DRAM)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Magnetic Disk&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;  SRAM -&amp;gt; DRAM -&amp;gt; Magnetic Disk로 갈수록 접근 속도가 100배 빨라지고 GB 당 가격이 100 배 낮아진다.&lt;/p&gt;
&lt;p&gt;빠른 메모리는 비싸기 때문에 이들을 적절히 혼용하여 가능한 빠른 성능을 내는 것이 최상의 선택이다.&lt;/p&gt;
&lt;h3 id="sramtechnology"&gt;SRAM Technology&lt;/h3&gt;
&lt;p&gt; 6 ~ 8 개의 트랜지스터를 사용하여 데이터를 저장한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;빠르지만 비쌈&lt;/li&gt;
&lt;li&gt;접근 시간이 일정&lt;/li&gt;
&lt;li&gt;Refresh 될 필요 없음&lt;/li&gt;
&lt;li&gt;캐시 메모리로 많이 사용됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="dramtechnology"&gt;DRAM Technology&lt;/h3&gt;
&lt;p&gt;  데이터는 Capacitor의 Charge로 저장됨&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;주기적으로 Refresh 되어야 함
&lt;ul&gt;
&lt;li&gt;Read Contents and Write Back&lt;/li&gt;
&lt;li&gt;Performed on a DRAM &amp;quot;row&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Synchronous DRAM (SDRAM)
&lt;ul&gt;
&lt;li&gt;대역폭을 향상하기 위해 클럭 추가&lt;/li&gt;
&lt;li&gt;Ex) Double Data Rate(DDR) DRAM : 클럭의 rising, falling edge에 데이터 전달&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="flashstorage"&gt;Flash Storage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;비휘발성 반도체 스토리지&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;디스크보다 100 ~ 1000배 빠름&lt;/li&gt;
&lt;li&gt;작고 저전력에 튼튼함&lt;/li&gt;
&lt;li&gt;근데 비쌈&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;종류&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NOR Flash&lt;/li&gt;
&lt;li&gt;NAND Flash&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flash bit는 1000번 이상 접근하면 닳는다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wear Leveling으로 최대한 고르게 사용하도록 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="diskmemory"&gt;Disk Memory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;부분 명칭 : Cylinder, Sector, Track, Disk Platters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;각각의 섹터에는..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sector ID&lt;/li&gt;
&lt;li&gt;Data&lt;/li&gt;
&lt;li&gt;Error Correcting Code(ECC)&lt;/li&gt;
&lt;li&gt;Synchronization Fields and Gaps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sector에 접근하기 위해서..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;다른 접근들은 큐에 대기시킴&lt;/li&gt;
&lt;li&gt;Seek : 트랙을 찾기 위해 헤드를 움직임&lt;/li&gt;
&lt;li&gt;Rotational Latency : 회전하는데 걸리는 시간&lt;/li&gt;
&lt;li&gt;Data Transfer : 데이터 전달&lt;/li&gt;
&lt;li&gt;Controller Overhead&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;평균 Read Time 계산&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;평균 Seek Time + 평균 Rotational Latency + Transfer Time + Controller Delay&lt;/li&gt;
&lt;li&gt;강의 자료 13 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="diskperformanceissues"&gt;Disk Performance Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;제조업체는 평균 Seek Time을 알려준다.&lt;/li&gt;
&lt;li&gt;Disk Drive는 캐시를 포함한다.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>5.1 Introduction</title><description>&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;(시간이 급한 관계로 몇 가지만 추려서 정리했습니다.)&lt;/p&gt;
&lt;h2 id="principleoflocality"&gt;Principle of Locality&lt;/h2&gt;
&lt;p&gt;  프로그램은 한 번에 작은 비율의 주소 공간에 접근한다. 그래서 다시 접근할 확률이 높은 것들에 대한 원칙을 정의하여 효율적으로 공간을 사용하는 것이 좋다.&lt;/p&gt;
&lt;h3 id="temporallocalitytime"&gt;Temporal Locality(Time)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;최근에 접근한 것들은 머지않아 다시 접근할 확률이 높다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="spatiallocalityspace"&gt;Spatial Locality(Space)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;최근에 접근한 것과&lt;/li&gt;&lt;/ul&gt;</description><link>http://identity16.github.io/5-1-introduction/</link><guid isPermaLink="false">5c35fc802d91b6c5113154ed</guid><category>computer-architecture</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Thu, 20 Dec 2018 16:57:00 GMT</pubDate><content:encoded>&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;(시간이 급한 관계로 몇 가지만 추려서 정리했습니다.)&lt;/p&gt;
&lt;h2 id="principleoflocality"&gt;Principle of Locality&lt;/h2&gt;
&lt;p&gt;  프로그램은 한 번에 작은 비율의 주소 공간에 접근한다. 그래서 다시 접근할 확률이 높은 것들에 대한 원칙을 정의하여 효율적으로 공간을 사용하는 것이 좋다.&lt;/p&gt;
&lt;h3 id="temporallocalitytime"&gt;Temporal Locality(Time)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;최근에 접근한 것들은 머지않아 다시 접근할 확률이 높다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="spatiallocalityspace"&gt;Spatial Locality(Space)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;최근에 접근한 것과 가까이 위치한 것이 다음에 접근할 확률이 높다.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>3.4 Conditioning on Event</title><description>&lt;h1 id="conditioningonevent"&gt;Conditioning on Event&lt;/h1&gt;
&lt;p&gt;  여기도 역시 Conditioning, 어떤 Event가 발생했을 때의 상황을 고려한 확률을 생각해볼 수 있다. 어떤 Event A가 발생한 조건에서 Continuous Random Variable $ X $의 PDF는 다음을 만족한다.&lt;/p&gt;
&lt;p&gt;$$ P(X\in B \vert A) = \displaystyle\int_B f_{X\vert A}(x)dx $$&lt;/p&gt;
&lt;p&gt;  만약 $ A $가 $ P(X \in&lt;/p&gt;</description><link>http://identity16.github.io/3-4-conditioning-on-event/</link><guid isPermaLink="false">5c35fcba2d91b6c5113154f2</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sun, 16 Dec 2018 05:34:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/chart-2785917_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="conditioningonevent"&gt;Conditioning on Event&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/chart-2785917_1280.jpg" alt="3.4 Conditioning on Event"&gt;&lt;p&gt;  여기도 역시 Conditioning, 어떤 Event가 발생했을 때의 상황을 고려한 확률을 생각해볼 수 있다. 어떤 Event A가 발생한 조건에서 Continuous Random Variable $ X $의 PDF는 다음을 만족한다.&lt;/p&gt;
&lt;p&gt;$$ P(X\in B \vert A) = \displaystyle\int_B f_{X\vert A}(x)dx $$&lt;/p&gt;
&lt;p&gt;  만약 $ A $가 $ P(X \in A) &amp;gt; 0 $인 실수 집합이면,&lt;/p&gt;
&lt;p&gt;$$ f_{X\vert A}(x) = \begin{cases} \frac{f_X(x)}{P(X\in A)}\ \ \ \ if\ x\in A \\0\ \ \ \ \ \ \ \ \ \ \  \ \ \ otherwise \end{cases} $$&lt;/p&gt;
&lt;h2 id="conditionalexpectation"&gt;Conditional Expectation&lt;/h2&gt;
&lt;p&gt;  이번에는 기대값이다. 위와 같은 조건에서 기대값은 아래와 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$ E[X\vert  A] = \displaystyle \int^{\infty}_{-\infty}xf_{X\vert A}(x)dx $$&lt;/p&gt;
&lt;p&gt;  2.6장에서 다룬 &amp;quot;Conditional Expectation&amp;quot;의 내용들이 여기서도 유효하다. 따라서 아래 식도 성립한다.&lt;/p&gt;
&lt;p&gt;$$ E[g(X)\vert A] = \displaystyle \int^{\infty}_{-\infty}g(x)f_{X\vert A}(x)dx $$&lt;/p&gt;
&lt;h2 id="versionoftotalprobabilitytheorem"&gt;Version of Total Probability Theorem&lt;/h2&gt;
&lt;p&gt;  $ A_1, \cdots, A_n $이 서로 disjoint하고 각각의 $ i $에 대해 $ P(A_i) &amp;gt; 0 $이면서 Sample Space의 Partition이라고 하자. 그러면 아래 식이 성립한다.&lt;/p&gt;
&lt;p&gt;$$ f_X(x) = \displaystyle \sum^n_{i=1} P(A_i)f_{X\vert A_i}(x) $$&lt;/p&gt;
&lt;p&gt;  이 식은 Total Probability Theorem을 이용해 $ P(X\le x) $를 구하는 식을 변형하여 증명할 수 있다.&lt;/p&gt;
</content:encoded></item><item><title>3.3 Normal Random Variables</title><description>&lt;h1 id="normalrandomvariable"&gt;Normal Random Variable&lt;/h1&gt;
&lt;p&gt;  Normal Random Variable의 PDF 그래프는 고등학교 때 한번쯤 봤을 법한 정규분포곡선이다.&lt;/p&gt;
&lt;p&gt;$$ f_X(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2} $$&lt;/p&gt;
&lt;p&gt;$$ E[X] = \mu $$&lt;/p&gt;
&lt;p&gt;$$ var(X) = \sigma ^2 $$&lt;/p&gt;
&lt;p&gt;$ \mu, \sigma $는 각각 평균과 표준편차를 나타내는 기호이다.&lt;/p&gt;
&lt;h2 id="properties"&gt;Properties&lt;/h2&gt;
&lt;p&gt;  Normal Random Variable은&lt;/p&gt;</description><link>http://identity16.github.io/3-3-normal-random-variables/</link><guid isPermaLink="false">5c35fd0f2d91b6c5113154fb</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sun, 16 Dec 2018 04:54:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/analytics-2158454_1280.png" medium="image"/><content:encoded>&lt;h1 id="normalrandomvariable"&gt;Normal Random Variable&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/analytics-2158454_1280.png" alt="3.3 Normal Random Variables"&gt;&lt;p&gt;  Normal Random Variable의 PDF 그래프는 고등학교 때 한번쯤 봤을 법한 정규분포곡선이다.&lt;/p&gt;
&lt;p&gt;$$ f_X(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2} $$&lt;/p&gt;
&lt;p&gt;$$ E[X] = \mu $$&lt;/p&gt;
&lt;p&gt;$$ var(X) = \sigma ^2 $$&lt;/p&gt;
&lt;p&gt;$ \mu, \sigma $는 각각 평균과 표준편차를 나타내는 기호이다.&lt;/p&gt;
&lt;h2 id="properties"&gt;Properties&lt;/h2&gt;
&lt;p&gt;  Normal Random Variable은 여러가지 특성을 가지고 있다. 물론 이것들도 대부분 고등학교 때 본 기억이 있을 것이다. 하나씩 살펴보자&lt;/p&gt;
&lt;h3 id="property1symmetry"&gt;Property 1: Symmetry&lt;/h3&gt;
&lt;p&gt;  확률 분포가 흔히 종모양 곡선을 그리며, 평균값을 기준으로  대칭을 이룬다. 예를 들어 $ \mu = 1 $인 Normal Random Variable이 있다고 하자. 이런 경우에는 1을 기준으로 확률 분포가 대칭이므로 $ P(x &amp;lt; 0) = P(x &amp;gt; 2) $ 이다. 뿐만 아니라 $ P(x \lt \mu) = P(x \gt \mu) = 0.5 $ 라는 사실도 대칭성에 의해 알 수 있다.&lt;/p&gt;
&lt;h3 id="property2normalityispreservedbylineartransformations"&gt;Property 2: Normality is Preserved by Linear Transformations&lt;/h3&gt;
&lt;p&gt; $ X $가 Normal Random Variable일때 $ Y = aX + b $인 Random Variable $ Y $도 여전히 Normal이고 $ X $의 평균과 분산이 각각  $ \mu, \sigma^2 $라 할 때 Y의 평균과 분산은 각각 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ E[Y] = a\mu + b $$&lt;/p&gt;
&lt;p&gt;$$ var(Y) = a^2\sigma^2 $$&lt;/p&gt;
&lt;h3 id="property3standardnormalrandomvariable"&gt;Property 3: Standard Normal Random Variable&lt;/h3&gt;
&lt;p&gt;  Standard Normal Random Variable이란 $ \mu = 0, \sigma^2 = 1$인 Normal Random Variable인데, 고등학교 때의 표준정규분포에 해당한다고 생각하면 된다.&lt;/p&gt;
&lt;h4 id="standardization"&gt;Standardization&lt;/h4&gt;
&lt;p&gt;  일반적인 Normal Random Variable $ X $를 Standard인 Random Variable $ Y $로 만드려면 아래와 같이 표준화(Standardization)할 수 있다.&lt;/p&gt;
&lt;p&gt;$$ Y = \frac{X - \mu}{\sigma} $$&lt;/p&gt;
&lt;h4 id="cdfofstandardnormalrandomvariable"&gt;CDF of Standard Normal Random Variable&lt;/h4&gt;
&lt;p&gt; Standard Normal Random Variable의 PDF는 아래와 같다. ($ \mu = 0, \sigma^2 = 1 $)&lt;/p&gt;
&lt;p&gt;$$ f_Y(y) = \frac{1}{\sqrt{2\pi}}e^{-y^2/2} $$&lt;/p&gt;
&lt;p&gt;  이를 바탕으로 CDF를 만들면 아래와 같다. 이 함수는 $ \Phi $으로 표기한다.&lt;/p&gt;
&lt;p&gt;$$ \Phi(y) = P(Y \le y) = P(Y \lt y) = \frac{1}{\sqrt{2\pi}}\int^{y}_{-\infty}e^{-t^2/2}dt $$&lt;/p&gt;
&lt;p&gt;$ \Phi $는 우리가 계산하지는 않고 Standard Normal Table을 보며 $ \Phi(y) $의 값을 찾아서 사용하면 된다.&lt;/p&gt;
</content:encoded></item><item><title>3.2 Cumulative Distribution Functions</title><description>&lt;h1 id="cumulativedistributionfunctions"&gt;Cumulative Distribution Functions&lt;/h1&gt;
&lt;p&gt;  연속적인 값에 대해, 우리는 $ P(X=x) $ 보다는 $ P(X \le x) $에 더 관심을 가지고 있다. PDF를 적분하면 그 값을 구할 수 있지만 $ f(x) = P(X \le x) $ 처럼 표현할 수 있는 함수가 있으면 더 편리할 것이다. 그런 함수가 Cumulative Distribution Function(CDF)이다. CDF는&lt;/p&gt;</description><link>http://identity16.github.io/3-2-cumulative-distribution-functions/</link><guid isPermaLink="false">5c35fd9f2d91b6c51131550a</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sun, 16 Dec 2018 04:27:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/stones-1694879_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="cumulativedistributionfunctions"&gt;Cumulative Distribution Functions&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/stones-1694879_1280.jpg" alt="3.2 Cumulative Distribution Functions"&gt;&lt;p&gt;  연속적인 값에 대해, 우리는 $ P(X=x) $ 보다는 $ P(X \le x) $에 더 관심을 가지고 있다. PDF를 적분하면 그 값을 구할 수 있지만 $ f(x) = P(X \le x) $ 처럼 표현할 수 있는 함수가 있으면 더 편리할 것이다. 그런 함수가 Cumulative Distribution Function(CDF)이다. CDF는 연속적인 값 뿐만 아니라 이산적인 값에 대한 식도 포함하고 있다.&lt;/p&gt;
&lt;p&gt;$$ F_X(x) = P(X \le x) = \begin{cases} \displaystyle\sum_{k \le x} p_X(k)\ \ \ \ \ \ \ X: discrete \\ \int^a_{-\infty}f_X(t)\ \ \ \ \ \ X: continuous \end{cases} $$&lt;/p&gt;
&lt;p&gt;  CDF는 0부터 1까지 증가하는 형태의 그래프로 표현된다. 랜덤 변수가 이산적인 값일 때는 계단형 그래프가 나올 것이고, 연속적일 때는 서서히 증가할 것이다.&lt;/p&gt;
&lt;h2 id="propertiesofthecdfs"&gt;Properties of the CDFs&lt;/h2&gt;
&lt;p&gt;  이제 CDF의 특성을 소개할 건데, Random Variable 종류에 따라 나누어 설명하겠다.&lt;/p&gt;
&lt;h3 id="discretervpropertiesofthecdfs"&gt;Discrete RV: Properties of the CDFs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CDF의 형태&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ F_X(k) = \displaystyle \sum^{k}_{i=-\infty} P(X = k) $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CDF로부터 PMF를 구하는 법&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ P(X=k) = F_{X}(k) - F_{X}(k-1) $$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="continuousrvpropertiesofthecdfs"&gt;Continuous RV: Properties of the CDFs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CDF의 형태&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ F_X(x) = \displaystyle \int^{x}_{-\infty} f_X(y)dy $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CDF로부터 PDF를 구하는 법&lt;/strong&gt; : CDF를 미분&lt;/p&gt;
&lt;p&gt;$$ f_X(x) = \frac{dF_X(x)}{dx} $$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>3.1 Continuous Random Variables and PDFs</title><description>&lt;h1 id="continuousrandomvariablesandpdfs"&gt;Continuous Random Variables and PDFs&lt;/h1&gt;
&lt;p&gt;  지금까지 우리는 이산적인 값에 대한 확률을 다루었다. 여기서는 연속적인 값에 대한 새로운 Random Variable과 Probability Density Function(PDF)를 소개하겠다.&lt;/p&gt;
&lt;h2 id="continuousrandomvariables"&gt;Continuous Random Variables&lt;/h2&gt;
&lt;p&gt;  Continuous Random Variable은 연속적인 값을 다루기 위한 것이며 다음과 같은 특징이 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;무한한 범위에서 정의되어있다.&lt;/li&gt;
&lt;li&gt;어떤 값 $ X=x $에 대하여, $ P(&lt;/li&gt;&lt;/ul&gt;</description><link>http://identity16.github.io/3-1-continuous-random-variables-and-pdfs/</link><guid isPermaLink="false">5c35fde72d91b6c511315511</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sun, 16 Dec 2018 04:00:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/black-and-white-2309273_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="continuousrandomvariablesandpdfs"&gt;Continuous Random Variables and PDFs&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/black-and-white-2309273_1280.jpg" alt="3.1 Continuous Random Variables and PDFs"&gt;&lt;p&gt;  지금까지 우리는 이산적인 값에 대한 확률을 다루었다. 여기서는 연속적인 값에 대한 새로운 Random Variable과 Probability Density Function(PDF)를 소개하겠다.&lt;/p&gt;
&lt;h2 id="continuousrandomvariables"&gt;Continuous Random Variables&lt;/h2&gt;
&lt;p&gt;  Continuous Random Variable은 연속적인 값을 다루기 위한 것이며 다음과 같은 특징이 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;무한한 범위에서 정의되어있다.&lt;/li&gt;
&lt;li&gt;어떤 값 $ X=x $에 대하여, $ P(X=x) = 0 $이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;  이 변수의 값이 될 수 있는 경우는 버스가 도착하는 시간, 무작위로 선택된 사람의 키 등이 있다.&lt;/p&gt;
&lt;h2 id="probabilitydensityfunctionpdf"&gt;Probability Density Function(PDF)&lt;/h2&gt;
&lt;p&gt;  Continuous Random Variable은 항상 $ P(X=x) = 0 $이기 때문에 PMF를 만드는 것은 실질적인 의미가 없다. PMF 대신 우리는 연속적이고 0 이상의 값을 가지는 Probability Density Function을 사용할 것이다. X에 대한 PDF는 $ f_X(x) $이라고 표기한다.&lt;/p&gt;
&lt;h3 id="intuition"&gt;Intuition&lt;/h3&gt;
&lt;p&gt;  아래는 PDF에 대한 이해를 돕기 위한 내용들이다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ f_X(a) $는 $ P(X=a) $가 아니다&lt;/li&gt;
&lt;li&gt;$ P(X=a) = \int^a_a f_x(x) dx = 0 $
&lt;ul&gt;
&lt;li&gt;=&amp;gt; $ P(X \le a) = P(X\lt a)+P(X=a) = P(X\lt A) $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;유효한 Probability Law는 $ P(\Omega) = 1 $과 $ P(A) \gt 0 $을 만족해야 한다고 했다. 이것은 PDF에도 비슷하게 적용된다.
&lt;ul&gt;
&lt;li&gt;Normalization, PDF 그래프의 총 밑넓이는 1이어야 한다.
&lt;ul&gt;
&lt;li&gt;$ \int^{\infty}_{-\infty}f_X(x) = P(-\infty &amp;lt; X &amp;lt; \infty) = 1 $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$ f_X $의 값들은 0 이상이어야 한다.
&lt;ul&gt;
&lt;li&gt;$ P(x\in B) = \int_{x\in B}f_X(x)dx \ge 0 $, for all $ B $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="propertiesofthepdf"&gt;Properties of the PDF&lt;/h3&gt;
&lt;p&gt;  아래는 PDF의 특성을 정리한 것이다. 여기서 $ X $는 Continuous Random Variable이다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ f_X(x) \ge 0,\ for\ all\ x $&lt;/li&gt;
&lt;li&gt;$ \int^{\infty}_{-\infty}f_X(x)dx = 1 $&lt;/li&gt;
&lt;li&gt;$ \delta $가 매우 작으면, $ P([x, x+\delta]) \approx f_X(x)\cdot \delta $&lt;/li&gt;
&lt;li&gt;실수 전체의 부분집합 $ B $에 대해
&lt;ul&gt;
&lt;li&gt;$ P(X\in B) = \int_B f_X(x)dx $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="expectationofacontinuousrandomvariable"&gt;Expectation of A Continuous Random Variable&lt;/h2&gt;
&lt;p&gt;  Continuous Random Variable의 기대값은 어떻게 구할까? 기존에 알고 있던 Random Variable들은 PMF를 사용해서 아래와 같이 Expectation을 구했다.&lt;/p&gt;
&lt;p&gt;$$ E[X] = \displaystyle \sum_{x} xp_X(x) $$&lt;/p&gt;
&lt;p&gt;  그렇지만 Continuous Random Variable은 PDF로 된 확률을 이용하고, 연속적인 값이기 때문에 $ \sum $ 대신 $ \int $를 이용하여 합친다.&lt;/p&gt;
&lt;p&gt;$$ E[X] = \int^{\infty}_{-\infty} xf_X(x)dx $$&lt;/p&gt;
&lt;h2 id="varianceofacontinuousrandomvariable"&gt;Variance of A Continuous Random Variable&lt;/h2&gt;
&lt;p&gt;  Continuous Random Variable이라고 해도 분산의 정의가 바뀌는 것은 아니기 때문에 앞에서 본 기대값을 적용만 하면 된다.&lt;/p&gt;
&lt;p&gt;$$ var(X) = E[(X - E[X])^2] = \int^{\infty}_{-\infty} (x-E[X])^2f_X(x)dx $$&lt;/p&gt;
&lt;h2 id="exponentialrandomvariable"&gt;Exponential Random Variable&lt;/h2&gt;
&lt;p&gt;  연속적인 값을 다루는 Random Variable인데 PDF의 형태가 지수함수이다.&lt;/p&gt;
&lt;p&gt;$$ f_X(x) = \begin{cases} \lambda e^{-\lambda x}\ \ if\ x\ge 0 \\ 0\ \ \ \ \ \ \ \ \ otherwise \end{cases} $$&lt;/p&gt;
&lt;p&gt;  기대값과 분산은 각각 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ E[X] = \frac{1}{\lambda} $$&lt;/p&gt;
&lt;p&gt;$$ var(X) = \frac{1}{\lambda^2} $$&lt;/p&gt;
&lt;p&gt;  그리고 $X$가 어떤 값 이상일 때의 확률은 지수 형태로 나온다.&lt;/p&gt;
&lt;p&gt;$$ P(X\ge a) = \int^{\infty}_{a} \lambda e^{-\lambda x} = -e^{-\lambda x} \bigg\rvert^{\infty}_a = e^{-\lambda a}$$&lt;/p&gt;
</content:encoded></item><item><title>2.7 Independence</title><description>&lt;h1 id="independence"&gt;Independence&lt;/h1&gt;
&lt;p&gt;  Random Variable 간의 독립인 경우에 대해 다루며, 이 때 PMF, Expectation, Variance가 어떤 특성을 지니는지도 함께 살펴볼 것이다.&lt;/p&gt;
&lt;h2 id="independenceofrandomvariables"&gt;Independence of Random Variables&lt;/h2&gt;
&lt;p&gt;  두 랜덤 변수 $ X, Y $가 독립이려면 PMF가 다음을 만족해야 한다.&lt;/p&gt;
&lt;p&gt;$$ p_{X,Y}(x,y) = p_X(x)p_Y(y)\ for\ all\ x,y$$&lt;/p&gt;
&lt;p&gt;  마치&lt;/p&gt;</description><link>http://identity16.github.io/2-7-independence/</link><guid isPermaLink="false">5c35fe742d91b6c51131551e</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sat, 15 Dec 2018 06:35:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/buttes-chaumont-898675_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="independence"&gt;Independence&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/buttes-chaumont-898675_1280.jpg" alt="2.7 Independence"&gt;&lt;p&gt;  Random Variable 간의 독립인 경우에 대해 다루며, 이 때 PMF, Expectation, Variance가 어떤 특성을 지니는지도 함께 살펴볼 것이다.&lt;/p&gt;
&lt;h2 id="independenceofrandomvariables"&gt;Independence of Random Variables&lt;/h2&gt;
&lt;p&gt;  두 랜덤 변수 $ X, Y $가 독립이려면 PMF가 다음을 만족해야 한다.&lt;/p&gt;
&lt;p&gt;$$ p_{X,Y}(x,y) = p_X(x)p_Y(y)\ for\ all\ x,y$$&lt;/p&gt;
&lt;p&gt;  마치 $ P(A\cap B) = P(A)P(B) $이면 두 Event가 독립인 것과 같은 이치이다. $ X,Y $가 서로의 확률에 영향을 미치지 않기 때문에 $ \{X=x \} $와 $\{Y=y\} $가 동시에 일어나는 경우의 확률은 각각의 확률을 곱한 것과 같다.&lt;/p&gt;
&lt;h2 id="expectationofindependentvariables"&gt;Expectation of Independent Variables&lt;/h2&gt;
&lt;p&gt;  기대값을 계산할 때 1. ($ E[XY] = \displaystyle \sum_x \sum_y xyp_{X,Y}(x,y) $ 라는 사실)에 2.(위에 나온 식)과 3.(기대값의 선형성)을 적용하면 다음과 같은 식이 탄생한다.&lt;/p&gt;
&lt;p&gt;$$ E[XY] = E[X]E[Y] $$&lt;/p&gt;
&lt;p&gt;위에서 1, 2, 3으로 적용 순서까지 적어놓았으니 직접 전개해서 이 식을 증명해보도록 하자.&lt;/p&gt;
&lt;h2 id="varianceofindependentvariables"&gt;Variance of Independent Variables&lt;/h2&gt;
&lt;p&gt;  독립인 두 랜덤 변수 $ X, Y $의 합 $ Z = X + Y $에 대해 다음 식이 성립한다.&lt;/p&gt;
&lt;p&gt;$$ var(Z) = var(X) + var(Y) $$&lt;/p&gt;
&lt;p&gt;  이것은 직관적이지 않으므로 증명을 한 번 해보자.&lt;/p&gt;
&lt;h3 id="proof"&gt;Proof&lt;/h3&gt;
&lt;p&gt;$$ \begin{equation}var(Z) \\ = E[(X + Y - E[X + Y])^2] \\= E[(X + Y - E[X] - E[Y])^2] \\= E[((X - E[X]) + (Y - E[Y]))^2]  \\= E[(X - E[X])^2] + E[(Y - E[Y])^2] +2E[(X-E[X])(Y-E[Y])] \\= E[(X - E[X])^2] + E[(Y - E[Y])^2] \\= var(X) + var(Y) \end{equation} $$&lt;/p&gt;
&lt;p&gt;  다섯 번째 줄에서 여섯 번째 줄로 넘어갈 때 $ 2E[(X-E[X])(Y-E[Y])] $가 사라졌는데, 이 값이 0이기 때문이다. 아래의 식을 보자. 참고로 $ X, Y $가 독립이므로 $ (X-E[X]) $ 와$ (Y-E[Y]) $도 독립이다.&lt;/p&gt;
&lt;p&gt;$$ \begin{equation} E[(X-E[X])(Y-E[Y])]\\=E[X-E[X]]E[Y-E[Y]]\\=(E[X]-E[E[X]])(E[Y]-E[E[Y]])\\=(E[X]-E[X])(E[Y]-E[Y])\\=0 \end{equation} $$&lt;/p&gt;
&lt;p&gt;여기까지, 2장이 전부 끝났다. 다음 포스팅부터는 3장에 들어갈 예정이다.&lt;/p&gt;
</content:encoded></item><item><title>2.6 Conditioning</title><description>&lt;h1 id="conditioning"&gt;Conditioning&lt;/h1&gt;
&lt;p&gt;  이 부분의 내용은 Random Variable에 조건부 확률 개념을 더하는 것이라고 생각하면 된다. 그에 따른 PMF, Expectation의 변화를 알아보자.&lt;/p&gt;
&lt;h2 id="conditioningonrandomvariableonanevent"&gt;Conditioning on Random Variable on an Event&lt;/h2&gt;
&lt;p&gt;  어떤 Event $ A $가 발생했을 때, Random Variable $ X $에 관한 PMF는 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ p_{X\vert A}(x) = P(X=x \vert A)&lt;/p&gt;</description><link>http://identity16.github.io/2-6-conditioning/</link><guid isPermaLink="false">5c35fed02d91b6c511315528</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sat, 15 Dec 2018 05:58:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/financial-2860753_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="conditioning"&gt;Conditioning&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/financial-2860753_1280.jpg" alt="2.6 Conditioning"&gt;&lt;p&gt;  이 부분의 내용은 Random Variable에 조건부 확률 개념을 더하는 것이라고 생각하면 된다. 그에 따른 PMF, Expectation의 변화를 알아보자.&lt;/p&gt;
&lt;h2 id="conditioningonrandomvariableonanevent"&gt;Conditioning on Random Variable on an Event&lt;/h2&gt;
&lt;p&gt;  어떤 Event $ A $가 발생했을 때, Random Variable $ X $에 관한 PMF는 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ p_{X\vert A}(x) = P(X=x \vert A) = \frac{P(\{X=x\} \cap A)}{P(A)} $$&lt;/p&gt;
&lt;p&gt;  사실 기존 조건부 확률과 크게 다를 것은 없다. 단지 Event를 표현하는 방식에 $ \{X=x\} $가 추가되었을 뿐이다.&lt;/p&gt;
&lt;h2 id="conditioningonrandomvariableonanother"&gt;Conditioning on Random Variable on Another&lt;/h2&gt;
&lt;p&gt;  이번에는 Event $ A $  대신 다른 Random Variable $ Y $에 대햐여 $ \{Y = y\} $ 일 때, Random Variable $ X $에 관한 PMF는 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ p_{X\vert Y}(x\vert y) = P(X=x \vert Y=y) = \frac{p_{X,Y}(x, y)}{p_Y(y)} $$&lt;/p&gt;
&lt;p&gt;$ A = \{Y=y\} $ 라고 생각하고 전개하면 된다. 둘 다 Random Variable이라서 마지막 수식은 Joint PMF와 PMF로 표현하였다.&lt;/p&gt;
&lt;h2 id="someusefulrules"&gt;Some Useful Rules&lt;/h2&gt;
&lt;p&gt;  여기서 소개하는 것은 Conditional PMF의 정의를 이용한 응용식이다.&lt;/p&gt;
&lt;h3 id="calculatingthejointpmffromtheconditionalpmf"&gt;Calculating the Joint PMF from the conditional PMF&lt;/h3&gt;
&lt;p&gt;  Conditional PMF로부터 Joint PMF를 구하는 식이다. 아마 이 식을 보면 조건부 확률에서 교집합의 확률을 구하는 식이 떠오를 것이다.&lt;/p&gt;
&lt;p&gt;$$ p_{X,Y}(x,y) = p_Y(y)p_{X\vert Y}(x\vert y) $$&lt;/p&gt;
&lt;p&gt;  이 식은 마치 조건부 확률에서의 $ P(A\cap B) = P(A)P(B\vert A) $와 유사하다.&lt;/p&gt;
&lt;h3 id="calculatingthemarginalpmffromtheconditionalpmf"&gt;Calculating the marginal PMF from the conditional PMF&lt;/h3&gt;
&lt;p&gt;  이번에는 Joint PMF가 아니라 Marginal PMF를 구하는 식이다.&lt;/p&gt;
&lt;p&gt;$$ p_X(x) = \displaystyle \sum_y p_Y(y)p_{X\vert Y}(x\vert y) $$&lt;/p&gt;
&lt;p&gt;  이것은 Total Probability Theorem과 유사하다.&lt;/p&gt;
&lt;h2 id="conditionalexpectation"&gt;Conditional Expectation&lt;/h2&gt;
&lt;p&gt;  조건이 들어간 상태에서 기대값이 어떻게 변하는지를 알아보자. 전제조건은 $ P(A) &amp;gt; 0 $이다.&lt;/p&gt;
&lt;p&gt;$$ E[X\vert A] = \displaystyle \sum_x xp_{X\vert A}(x\vert A) $$&lt;/p&gt;
&lt;p&gt; $ X $ 대신 함수인 $ g(X) $가 들어가도 큰 차이 없이,&lt;/p&gt;
&lt;p&gt;$$ E[g(X)\vert A] = \displaystyle \sum_x g(x)p_{X\vert A}(x\vert A) $$&lt;/p&gt;
&lt;p&gt;이고, $ A $ 대신 $\{Y=y\} $라면&lt;/p&gt;
&lt;p&gt;$$ E[X\vert Y=y] = \displaystyle \sum_x xp_{X\vert Y}(x\vert y) $$&lt;/p&gt;
&lt;p&gt;로 기대값을 구할 수 있다.&lt;/p&gt;
&lt;h3 id="totalexpectationtheorem"&gt;Total Expectation Theorem&lt;/h3&gt;
&lt;p&gt;  Total Probability Theorem의 기대값 버전이라고 생각하면 된다. 기본적인 개념은 모든 $ Y $ 값과 알고자하는 $ X $ 값에 대한 조건부 기대값들을 다시 평균 내는 것이다.&lt;/p&gt;
&lt;p&gt;$$ E[X] = \displaystyle \sum_y p_Y(y)E[X\vert Y=y] $$&lt;/p&gt;
</content:encoded></item><item><title>2.4 Expectation, Mean, and Variance &amp; 2.5 Joint PMFs of Multiple</title><description>&lt;h1 id="expectationmeanandvariance"&gt;Expectation, Mean, and Variance&lt;/h1&gt;
&lt;p&gt;  여기서는 예전에 설명했던 Random Variable의 종류들에 대한 Expectation과 Variance가 어떻게 되는지만 간단하게 말해줄 것이다. 단, 증명은 단순히 전개하면 알 수 있으므로 생략한다.&lt;/p&gt;
&lt;h2 id="expectationsofstandardrandomvariables"&gt;Expectations of Standard Random Variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Uniform&lt;/strong&gt; on $ {a, a + 1, \dots, b} $&lt;/p&gt;
&lt;p&gt;$$ E[X] = \frac{a+b}{2} $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ E[X] = (1-p)\cdot&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</description><link>http://identity16.github.io/2-4-expectation-mean-and-variance-2-5-joint-pmfs-of-multiple/</link><guid isPermaLink="false">5c35ff612d91b6c511315537</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sat, 15 Dec 2018 05:16:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/bayesian-2889576_1280.png" medium="image"/><content:encoded>&lt;h1 id="expectationmeanandvariance"&gt;Expectation, Mean, and Variance&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/bayesian-2889576_1280.png" alt="2.4 Expectation, Mean, and Variance &amp; 2.5 Joint PMFs of Multiple"&gt;&lt;p&gt;  여기서는 예전에 설명했던 Random Variable의 종류들에 대한 Expectation과 Variance가 어떻게 되는지만 간단하게 말해줄 것이다. 단, 증명은 단순히 전개하면 알 수 있으므로 생략한다.&lt;/p&gt;
&lt;h2 id="expectationsofstandardrandomvariables"&gt;Expectations of Standard Random Variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Uniform&lt;/strong&gt; on $ {a, a + 1, \dots, b} $&lt;/p&gt;
&lt;p&gt;$$ E[X] = \frac{a+b}{2} $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ E[X] = (1-p)\cdot 0 + p\cdot 1 = p $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Binomial&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ E[X] = \sum^{n}_{k=0} k\cdot {n \choose k} p^k (1-p)^{n-k} = np $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Geometric&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ E[X] = \sum^{\infty}_{k=1}k\cdot (1-p)^{k-1}p = \frac{1}{p} $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Poisson&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$E[X] = \sum^{\infty}_{k=0} k\cdot \frac{e^{-\lambda}}{k!} = \lambda $$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="variancesofstandardrandomvariables"&gt;Variances of Standard Random Variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Uniform&lt;/strong&gt; on $ {a, a + 1, \dots, b} $&lt;/p&gt;
&lt;p&gt;$$var(X) = \frac{(b-a+1)^2 - 1}{12}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ var(X) = p(1-p) $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Binomial&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ var(X) =  np(1-p) $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Geometric&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$var(X) = \frac{1-p}{p^2}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Poisson&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$var(X) = \lambda $$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="jointpmfsofmultiplerandomvariables"&gt;Joint PMFs of Multiple Random Variables&lt;/h1&gt;
&lt;h2 id="jointpmf"&gt;Joint PMF&lt;/h2&gt;
&lt;p&gt;두 개의 Discrete Random Variable $ X, Y $가 같은 시행에 연관되어 있을 때, Joint PMF는 다음과 같이 정의될 수 있다.&lt;/p&gt;
&lt;p&gt;$$ p_{X,Y}(x,y) = P(X=x, Y=y) $$&lt;/p&gt;
&lt;p&gt;  $P(X=x, Y=y) $는 $P(X=x\ and\ Y=y) $나 $P(\{X=x\} \cap \{Y=y\}) $와 같은 뜻이다.&lt;/p&gt;
&lt;p&gt;  Joint PMF는 하나의 시행에서 여러 속성을 통해 결과를 표현할 때 유용하다. 예를 들어 $ X $는 키,  $ Y $는 성적으로 두고 학생을 랜덤하게 뽑는 경우를 $ P(X=160, Y=80) $과 같이 표현할 수 있다.&lt;/p&gt;
&lt;h2 id="marginalpmf"&gt;Marginal PMF&lt;/h2&gt;
&lt;p&gt;  Joint PMF $ p_{X, Y}(x, y) $에서 각각의 랜덤 변수 $X, Y$에 대한 PMF를 뽑아내면 그것을 Marginal PMF라 부르고 다음과 같이 구할 수 있다.&lt;/p&gt;
&lt;p&gt;$$ p_X(x) = \displaystyle \sum_y p_{X,Y}(x,y) $$&lt;/p&gt;
&lt;p&gt;$$ p_Y(y) = \displaystyle \sum_x p_{X,Y}(x,y) $$&lt;/p&gt;
&lt;p&gt;  어떤 $ x $ 또는 $ y $에 대해 나머지 랜덤 변수의 모든 경우 확률의 합이다.&lt;/p&gt;
&lt;h2 id="functionsofmultiplerandomvariables"&gt;Functions of Multiple Random Variables&lt;/h2&gt;
&lt;p&gt;  여기서는 이전 포스팅의 $ E[g(X)] = \displaystyle \sum_x g(x)p_X(x) $를 Joint PMF에 대한 것으로 확장한 것이다. 형태는 같고 단지 랜덤 변수 $ Y $ 가 추가되었을 뿐이다. 아래는 확장된 식이다.&lt;/p&gt;
&lt;p&gt;$$ E[g(X, Y)] = \displaystyle \sum_{x, y} g(x, y)p_{X, Y}(x, y) $$&lt;/p&gt;
&lt;p&gt;  $ g(X, Y) = aX+bY+c $인  경우, $ E[g(X, Y)] $는 지난 번에 이야기한 선형성에 의해 다음과 같다.&lt;/p&gt;
&lt;p&gt;$$ E[g(X, Y)] = aE[X] + bE[Y] + c $$&lt;/p&gt;
</content:encoded></item><item><title>2.3 Functions of Random Variables &amp; 2.4 Expectation, Mean, and Variance</title><description>&lt;h1 id="functionsofrandomvariables"&gt;Functions of Random Variables&lt;/h1&gt;
&lt;p&gt;  여기서는 Random Variable이 값이 아닌 함수로 정의되는 경우에 대해 이야기한다.&lt;/p&gt;
&lt;p&gt;Random Variable $ X $와 $ f : \mathbb{R} \rightarrow \mathbb{R} $인 함수 $ f $가 있다고 하자. 그리고 새로운 Random Variable $ Y $를 다음과 같이 정의할 수 있다.&lt;/p&gt;
&lt;p&gt;$$ Y = f(X) $$&lt;/p&gt;
&lt;p&gt;  이런 $ Y $의 PMF는 아래와 같다.&lt;/p&gt;</description><link>http://identity16.github.io/2-3-functions-of-random-variables-2-4-expectation-mean-and-variance/</link><guid isPermaLink="false">5c36000e2d91b6c511315549</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sat, 15 Dec 2018 04:30:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/graphic-1606688_1280.png" medium="image"/><content:encoded>&lt;h1 id="functionsofrandomvariables"&gt;Functions of Random Variables&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/graphic-1606688_1280.png" alt="2.3 Functions of Random Variables &amp; 2.4 Expectation, Mean, and Variance"&gt;&lt;p&gt;  여기서는 Random Variable이 값이 아닌 함수로 정의되는 경우에 대해 이야기한다.&lt;/p&gt;
&lt;p&gt;Random Variable $ X $와 $ f : \mathbb{R} \rightarrow \mathbb{R} $인 함수 $ f $가 있다고 하자. 그리고 새로운 Random Variable $ Y $를 다음과 같이 정의할 수 있다.&lt;/p&gt;
&lt;p&gt;$$ Y = f(X) $$&lt;/p&gt;
&lt;p&gt;  이런 $ Y $의 PMF는 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ P(Y=k) = P(f(X) = k) = \sum_{o\in \Omega\ such\ that\ f(X(o))=k}P(o) $$&lt;/p&gt;
&lt;p&gt; 풀어서 설명하면 $ {Y=k} $의 확률은 $ f(X)=k $가 되게하는 outcome $ o $들의 확률을 더한 것이다.&lt;/p&gt;
&lt;h1 id="expectationmeanandvariance"&gt;Expectation, Mean, and Variance&lt;/h1&gt;
&lt;h2 id="expectedvalue"&gt;Expected Value&lt;/h2&gt;
&lt;p&gt;  Expected Value, 기대값은 Random Variable $ X $에 대해 다음과 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$ E[X] = \displaystyle \sum_{x\in \mathbb{R}} xP(X=x) $$&lt;/p&gt;
&lt;p&gt;이 값은 $X$ 값에 대한 확률을 고려한 평균이라고 할 수 있다. $ E[X] $는 Expectation 또는 Mean이라고도 불린다.&lt;/p&gt;
&lt;h3 id="linearityofexpectation"&gt;Linearity of Expectation&lt;/h3&gt;
&lt;p&gt;  Expectation은 선형성이라는 특성을 가지고 있다. 고등학교 때 들어본 경험이 있을 수도 있는데 아래 식과 같은 특성을 가진 것을 선형성이 있다고 한다.&lt;/p&gt;
&lt;p&gt;$$ E[X + Y] = E[X] + E[Y] $$&lt;/p&gt;
&lt;p&gt;$$ E[aX] = aE[X] $$&lt;/p&gt;
&lt;h2 id="variance"&gt;Variance&lt;/h2&gt;
&lt;p&gt;  우리가 분산이라고 부르는 Variance는 평균으로부터 얼마나 떨어져 있는지를 계산한 값이다. 이는 모든 $X$에 대해 $ (X-E[X])^2 $ 를 구한 다음 평균을 내서 구한다. 아래에 식으로 정리되어 있다.&lt;/p&gt;
&lt;p&gt;$$ var(X) = E[(X-E[X])^2] = \sum(k-E[X])^2 P(X=k) $$&lt;/p&gt;
&lt;p&gt;Variance를 정리하면 아래 식과도 같다.&lt;/p&gt;
&lt;p&gt;$$ var(X) = E[X^2] - E[X]^2 $$&lt;/p&gt;
&lt;p&gt;  고등학교 때 '제평평제' 등으로 외웠던 식이다.&lt;/p&gt;
&lt;p&gt;그리고 분산으로부터 &lt;em&gt;표준편차&lt;/em&gt;도 구할 수 있는데, 분산에 루트를 씌우면 된다.&lt;/p&gt;
&lt;p&gt;$$ \sigma_x = std(X) = \sqrt{var(X)} $$&lt;/p&gt;
&lt;h2 id="expectedvalueruleforfunctionsofrandomvariable"&gt;Expected Value Rule for Functions of Random Variable&lt;/h2&gt;
&lt;p&gt;  앞서 Random Variable이 함수로 정의될 수도 있다고 했다. 여기서는 Random Variable 함수의 Expectation을 구하는 식을 소개한다. $ g(X) $는 Random Variable을 실수로 매핑하는 함수이다. 이 함수의 기대값은 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ E[g(X)] = \displaystyle \sum_x g(x)p_X(x) $$&lt;/p&gt;
&lt;h2 id="meanandvarianceofalinearfunctionofarandomvariable"&gt;Mean and Variance of a Linear Function of a Random Variable&lt;/h2&gt;
&lt;p&gt;  Random Variable이 선형 함수 형태일 때, Expectation과 Variance는 어떻게 변할까? 선형 함수를 일반화한 Random Variable $ Y=aX+b $를 기준으로 다음과 같다.&lt;/p&gt;
&lt;p&gt;$$ E[Y] = aE[X]+b $$&lt;/p&gt;
&lt;p&gt;$$ var(Y) = a^2var(X) $$&lt;/p&gt;
&lt;p&gt;Expectation은 앞에서 말한 선형성 때문임을 알 수 있고, Variance는 전개를 직접해서 왜 그런지 증명해보자. 그러면 다음부턴 당연하게 여겨질 것이다.&lt;/p&gt;
</content:encoded></item><item><title>2.1 Basic Concepts &amp; 2.2 Probability Mass Function</title><description>&lt;p&gt;  2장의 전체적인 내용은 Discrete Random Variable에 관한 것이다. 이 포스팅에서는 Random Variable과 Probability Mass Function(PMF)이 무엇인지에 대해 설명한다.&lt;/p&gt;
&lt;h1 id="basicconcepts"&gt;Basic Concepts&lt;/h1&gt;
&lt;h2 id="randomvariable"&gt;Random Variable&lt;/h2&gt;
&lt;p&gt;  정의를 그대로 읽어보자면, Random Variable이란 Sample Space를 실수에 매핑하는 함수이다. 다시 말해 입력은 outcome, 출력은 실수인 함수이다. 아직은 말이 어렵다. 예제를 통해 알아보자.&lt;/p&gt;
&lt;h3 id="example"&gt;Example&lt;/h3&gt;
&lt;p&gt;  4면을 가지고(&lt;/p&gt;</description><link>http://identity16.github.io/2-1-basic-concepts-2-2-probability-mass-function/</link><guid isPermaLink="false">5c3600a72d91b6c511315559</guid><category>probability-and-statistics</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Sat, 15 Dec 2018 03:45:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/preview.jpg" medium="image"/><content:encoded>&lt;img src="http://identity16.github.io/content/images/2019/01/preview.jpg" alt="2.1 Basic Concepts &amp; 2.2 Probability Mass Function"&gt;&lt;p&gt;  2장의 전체적인 내용은 Discrete Random Variable에 관한 것이다. 이 포스팅에서는 Random Variable과 Probability Mass Function(PMF)이 무엇인지에 대해 설명한다.&lt;/p&gt;
&lt;h1 id="basicconcepts"&gt;Basic Concepts&lt;/h1&gt;
&lt;h2 id="randomvariable"&gt;Random Variable&lt;/h2&gt;
&lt;p&gt;  정의를 그대로 읽어보자면, Random Variable이란 Sample Space를 실수에 매핑하는 함수이다. 다시 말해 입력은 outcome, 출력은 실수인 함수이다. 아직은 말이 어렵다. 예제를 통해 알아보자.&lt;/p&gt;
&lt;h3 id="example"&gt;Example&lt;/h3&gt;
&lt;p&gt;  4면을 가지고(각 면의 숫자는 1, 2, 3, 4) 모든 면이 나올 확률이 동일한 두 개의 주사위가 있다. 두 주사위를 동시에 던졌을 때 나오는 숫자쌍을 $ o\in{\Omega}, o = (o_1, o_2) $라 하자. 이를 바탕으로 $ X(o_1, o_2) = max(o_1, o_2) $로 정의된 함수를 생각해볼 수 있다. 이 때 $X$는 Sample Space의 $o$를 실수인 $ o_1, o_2 $ 중 하나로 매핑하는 것으로 볼 수 있으므로 Random Variable인 것이다.&lt;/p&gt;
&lt;h2 id="randomvariablesgiveaneasywaytospecifyevents"&gt;Random Variables Give An Easy Way to Specify Events&lt;/h2&gt;
&lt;p&gt;  랜덤 변수를 사용하면 어떤 Event를 아주 단순하게 지칭할 수 있게 된다. $ X : \Omega \rightarrow \mathbb{R} $ 인 함수(or Random Variable)가 있을 때 다음과 같이 Event를 표현할 수 있다.&lt;/p&gt;
&lt;p&gt;$$ {X = x} = {o \vert o \in \Omega\ and\ X(o) = x} $$&lt;/p&gt;
&lt;p&gt;  즉, $X$가 $x$가 되게하는 Event를 $ X=x $라는 것으로 표현할 수 있다는 것이다. 주사위 예제에서 한 번 적용해보자면, $ {(1,2), (2,1), (2,2)} $ 라는 Event를 표현하기 위해서 $ {X=2} $라고 단순하게 적을 수 있게 된 것이다. 2라는 숫자가 함수 $X$에 의해 $ {(1,2), (2,1), (2,2)} $와 매핑되었기 때문에 가능해졌다.&lt;/p&gt;
&lt;h2 id="randomvariablesandprobability"&gt;Random Variables and Probability&lt;/h2&gt;
&lt;p&gt; 확률을 표현할 때도 Random Variable을 사용하면 편리하다. 주사위 예제를 계속 활용하여 설명하겠다. Random Variable을 사용하면 $ (1,2), (2,1), (2,2) $ 다음 세 outcome의 확률을 더한 값을 알고 싶을 때 $ P(X=2) $라고만 쓰면 된다.&lt;/p&gt;
&lt;h1 id="probabilitymassfunctionpmf"&gt;Probability Mass Function(PMF)&lt;/h1&gt;
&lt;p&gt;  Probability Mass Function이란 ${X=x}$의 확률에 대한 함수이다. 즉 입력은 Random Variable 값이고 출력은 확률으로 가지는 함수인 것이다. 바로 앞 주제에서 본 형태인 $ P(X=x) $ 또는 간단하게 $ p_X(x) $로 나타낸다.&lt;/p&gt;
&lt;h1 id="randomvariable"&gt;Random Variable의 종류&lt;/h1&gt;
&lt;p&gt;  Random Variable은 우리가 임의로 정의할 수 있지만, 많은 상황에서 적용할 수 있는 대표적인 Random Variable의 종류를 소개하겠다.&lt;/p&gt;
&lt;h2 id="discreteuniformrandomvariables"&gt;Discrete Uniform Random Variables&lt;/h2&gt;
&lt;p&gt;  특정 범위(a와 b 사이)의 Random Variable 값이 같은 확률을 가지는 경우이다. 그래서 Discrete Uniform Random Variable의 PMF는 아래와 같다.&lt;/p&gt;
&lt;p&gt;$$ p_X(k) = P(X = k) =  \begin{cases}&lt;br&gt;
\frac{1}{b-a-1}\ if\ k=a, a+1, \cdots, b\\&lt;br&gt;
0\ if\ otherwise&lt;br&gt;
\end{cases} $$&lt;/p&gt;
&lt;p&gt;$ \frac{1}{b-a-1} $은 a와 b 사이의 정수들이 가지는 동일한 확률이다.&lt;/p&gt;
&lt;h2 id="bernoullirandomvariables"&gt;Bernoulli Random Variables&lt;/h2&gt;
&lt;p&gt;  베르누이 랜덤 변수는 값을 두 개만 가진다. 동전 토스 같이 두 가지 outcome만 있는 상황에서 사용된다. PMF는 다음과 같다.($X$는 0 또는 1)&lt;/p&gt;
&lt;p&gt;$$ p_X(k) = P(X=k) = \begin{cases} p\ \ \ \ \ \ \ \ \ \ \ if\ k=1 \\ 1-p\ \ \ \ if\ k=0 \end{cases} $$&lt;/p&gt;
&lt;h2 id="binomialrandomvariables"&gt;Binomial Random Variables&lt;/h2&gt;
&lt;p&gt;  성공 / 실패가 있는 상황이 n번 시행되었을 때 성공한 횟수를 $ X $의 값으로 가지는 Random Variable이다. 한 번의 시행에서 성공할 확률을 $ p $라고 하자. PMF는 우리가 흔히 아는 형태로 다음과 같다.&lt;/p&gt;
&lt;p&gt;$$ p_X(k) = P(X=k) = {n \choose k} p^k (1-p)^{n-k}\ (k=0, 1, \dots, n) $$&lt;/p&gt;
&lt;h2 id="geometricrandomvariables"&gt;Geometric Random Variables&lt;/h2&gt;
&lt;p&gt;  몇 번째 시행에서 처음 성공했는지를 $ X $의 값으로 가진다. $ X = k $라 하면, 첫 번째부터 k-1 번째 시행은 전부 실패하고 k 번째에서 성공했다는 뜻이다. 한 번의 시행에 대해 성공할 확률을 $ p $라고 하면, PMF는 다음과 같다.&lt;/p&gt;
&lt;p&gt;$$ p_X(x) = P(X=k) = (1-p)^{k-1}p $$&lt;/p&gt;
&lt;h2 id="poissonrandomvariables"&gt;Poisson Random Variables&lt;/h2&gt;
&lt;p&gt;  Binomial Random Variable에서 n이 매우 크고 p가 매우 작은 상황에서는 연산 비용이 매우 커진다. Poisson Random Variable은 이러한 상황에서 적용할 수 있는 Binomial Random Variable의 PMF를 근사시켜 연산 비용을 줄인 PMF를 가지고 있다. 따라서 기본적인 상황 개념과 X의 값은 Binomial과 동일하다. 아래는 PMF인데, 여기서 $ \lambda = np $이다.&lt;/p&gt;
&lt;p&gt;$$ p_X(k) = p(X=k) = e^{-\lambda}\frac{\lambda^{k}}{k!} \approx  {n \choose k} p^k (1-p)^{n-k}$$&lt;/p&gt;
&lt;!--Image Credit(Freepik) --&gt;
&lt;a style="font-family:sans-serif; font-size: 15px; color: #26a8ed;
    text-decoration: none; box-shadow: none;" href="https://www.freepik.com/free-photos-vectors/background"&gt;Header Image created by Freepik&lt;/a&gt;</content:encoded></item><item><title>4.14 Fallacies and Pitfalls</title><description>&lt;h1 id="fallaciesandpitfalls"&gt;Fallacies and Pitfalls&lt;/h1&gt;
&lt;h2 id="fallacypipelineiseasy"&gt;Fallacy: Pipeline is easy&lt;/h2&gt;
&lt;p&gt;  지금까지 파이프라인을 잘 이해한 사람들은 어쩌면 이것이 별 거 아니라고 생각할 수도 있다. 그렇지만 지금까지 다룬 것은 파이프라인의 기본적인 개념이고, 더 세부적인 것을 고려하기 시작하면 만만치 않을 것이다.&lt;/p&gt;
&lt;p&gt;  파이프라인의 어려움에 대한 이야기를 말하자면, 이 강의 교재의 1st Edition에서 나온 파이프라인에 버그가 있었는데 이&lt;/p&gt;</description><link>http://identity16.github.io/4-14-fallacies-and-pitfalls/</link><guid isPermaLink="false">5c35fb602d91b6c5113154dd</guid><category>computer-architecture</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Fri, 14 Dec 2018 13:50:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/14231.jpg" medium="image"/><content:encoded>&lt;h1 id="fallaciesandpitfalls"&gt;Fallacies and Pitfalls&lt;/h1&gt;
&lt;h2 id="fallacypipelineiseasy"&gt;Fallacy: Pipeline is easy&lt;/h2&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/14231.jpg" alt="4.14 Fallacies and Pitfalls"&gt;&lt;p&gt;  지금까지 파이프라인을 잘 이해한 사람들은 어쩌면 이것이 별 거 아니라고 생각할 수도 있다. 그렇지만 지금까지 다룬 것은 파이프라인의 기본적인 개념이고, 더 세부적인 것을 고려하기 시작하면 만만치 않을 것이다.&lt;/p&gt;
&lt;p&gt;  파이프라인의 어려움에 대한 이야기를 말하자면, 이 강의 교재의 1st Edition에서 나온 파이프라인에 버그가 있었는데 이 버그는 100명 이상이 검토하고 18개 대학에서 시험을 칠 동안 발견되지 않았다. 어떤 사람이 이 책을 바탕으로 컴퓨터를 만드려고 시도하다가 그제서야 이 버그가 발견되었다고 한다.&lt;/p&gt;
&lt;h2 id="fallacypipeliningcanbeimplementedindependentoftechnology"&gt;Fallacy: Pipelining can be implemented independent of technology&lt;/h2&gt;
&lt;p&gt;  위 문장은 한국어로 &amp;quot;파이프라인은 기술과 상관 없이 구현 가능하다&amp;quot;는 착각인데 그렇지 않다. 칩에 들어가는 트랜지스터가 많아질수록 더 발전된 파이프라인을 만들 수 있으며, 파이프라인과 관련된 ISA 설계는 많은 기술 트렌드를 필요로 한다.&lt;/p&gt;
&lt;h2 id="pitfallfailuretoconsiderinstructionsetdesigncanadverselyimpactpipelining"&gt;Pitfall: Failure to consider instruction set design can adversely impact pipelining&lt;/h2&gt;
&lt;p&gt;  파이프라이닝을 하기에 얼마나 힘든지는 Instruction Set의 설계를 잘했는지 보다는 얼마나 복잡한 Instruction Set인지에 영향을 많이 받는다. 복잡한 Instruction들은 파이프라이닝을 구현하기 위해서 상당한 오버헤드가 존재하기 때문이다.&lt;/p&gt;
&lt;!--Image Credit(Freepik) --&gt;
&lt;a style="font-family:sans-serif; font-size: 15px; color: #26a8ed;
    text-decoration: none; box-shadow: none;" href="https://www.freepik.com/free-photos-vectors/background"&gt;Header Image created by Freepik&lt;/a&gt;</content:encoded></item><item><title>4.7 Data Hazards: Forwarding versus Stalling</title><description>&lt;h1 id="datahazardsforwardingversusstalling"&gt;Data Hazards: Forwarding versus Stalling&lt;/h1&gt;
&lt;p&gt;  지금까지 구현한 회로는 Hazard가 발생하지 않는 상황에서는 잘 동작한다. 하지만 실제 프로그램에서는 Hazard가 많이 발생하기 때문에 이를 처리할 수 있도록 고쳐서 조금 더 현실적인 프로세서를 만들어보자.&lt;/p&gt;
&lt;h2 id="forwarding"&gt;Forwarding&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-assembly"&gt;sub   $2, $1,$3 # Register $2 written by sub
and   $12,$2,$5 # 1st operand($2) depends on&lt;/code&gt;&lt;/pre&gt;</description><link>http://identity16.github.io/4-7-data-hazards-forwarding-versus-stalling/</link><guid isPermaLink="false">5c35fac82d91b6c5113154cf</guid><category>computer-architecture</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Fri, 14 Dec 2018 13:22:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/07.jpg" medium="image"/><content:encoded>&lt;h1 id="datahazardsforwardingversusstalling"&gt;Data Hazards: Forwarding versus Stalling&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/07.jpg" alt="4.7 Data Hazards: Forwarding versus Stalling"&gt;&lt;p&gt;  지금까지 구현한 회로는 Hazard가 발생하지 않는 상황에서는 잘 동작한다. 하지만 실제 프로그램에서는 Hazard가 많이 발생하기 때문에 이를 처리할 수 있도록 고쳐서 조금 더 현실적인 프로세서를 만들어보자.&lt;/p&gt;
&lt;h2 id="forwarding"&gt;Forwarding&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-assembly"&gt;sub   $2, $1,$3 # Register $2 written by sub
and   $12,$2,$5 # 1st operand($2) depends on sub
or    $13,$6,$2 # 2nd operand($2) depends on sub
add   $14,$2,$2 # 1st($2) &amp;amp; 2nd($2) depend on sub
sw    $15,100($2) # Base ($2) depends on sub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;  위 코드의 &lt;code&gt;$2&lt;/code&gt;를 보자. &lt;code&gt;sub&lt;/code&gt;에서 처음 값이 저장된 후, 다음 4 개의 instruction에서 읽기 레지스터로 사용된다. 낭비되는 사이클 없이 파이프라인에서 실행된다면 Multi-cycle-pipeline 다이어그램은 아래 이미지 &lt;em&gt;(강의 교재 figure 4.52)&lt;/em&gt; 와 같을 것이다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-7-1.png" alt="4.7 Data Hazards: Forwarding versus Stalling"&gt;&lt;/p&gt;
&lt;p&gt;  여기서 파란색은 &lt;code&gt;$2&lt;/code&gt;를 쓰거나 읽는 부분인데, 첫 줄에서 &lt;code&gt;$s2&lt;/code&gt;가 저장되기 전에 &lt;code&gt;$2&lt;/code&gt;를 읽는 곳은 &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt; 두 군데이다. 즉, Data Hazard가 발생했으며 이를 해결하기 위해서는 Forwarding이 필요하다. 이 경우에는 &lt;code&gt;sub&lt;/code&gt; 의 EX 단계 ALU 결과를 &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;의 EX 입력으로 연결해 주어야 한다.&lt;/p&gt;
&lt;p&gt;  그렇다면 Data Hazard가 발생했는지는 어떻게 알 수 있을까? 그 방법은 각 단계의 사이에 있는 파이프라인 레지스터(ID/EX 등)에 저장되어 있는 레지스터 값을 비교하는 것이다. 다음 네 가지 경우이면 Data Hazard이다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EX/MEM의 R&lt;sub&gt;d&lt;/sub&gt; = ID/EX의 R&lt;sub&gt;s&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;EX/MEM의 R&lt;sub&gt;d&lt;/sub&gt; = ID/EX의 R&lt;sub&gt;t&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;MEM/WB의 R&lt;sub&gt;d&lt;/sub&gt; = ID/EX의 R&lt;sub&gt;s&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;MEM/WB의 R&lt;sub&gt;d&lt;/sub&gt; = ID/EX의 R&lt;sub&gt;t&lt;/sub&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;  항상 저 값들을 비교하는 것은 아닌데, 간혹가다 EX/MEM이나 MEM/WB의 R&lt;sub&gt;d&lt;/sub&gt;에 값이 저장되지 않는 경우에는 포워딩을 하면 안되기 때문이다. EX/MEM과 MEM/WB에 RegWrite라는 신호를 추가하면 해결할 수 있다. 그리고 R&lt;sub&gt;d&lt;/sub&gt;가 &lt;code&gt;$0&lt;/code&gt;(항상 값이 0이어야 하는 레지스터)이면 값을 저장해선 안되기 때문에 EX/MEM의 R&lt;sub&gt;d&lt;/sub&gt; != 0, MEM/WB의 R&lt;sub&gt;d&lt;/sub&gt; != 0 조건까지 추가한다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-7-2.png" alt="4.7 Data Hazards: Forwarding versus Stalling"&gt;&lt;/p&gt;
&lt;p&gt;  위 이미지 &lt;em&gt;(강의 교재 figure 4.54)&lt;/em&gt; 는 Forwarding을 고려해 변경한 회로이다. ALU 입력값 MUX에 포워딩 된 값이 추가되었다. 그리고 포워딩 상태를 다루기 위한 Forwarding Unit도 더해졌다.&lt;/p&gt;
&lt;p&gt;  Forwarding Unit에서 ForwardA와 ForwardB의 값을 정하는 기준을 살펴보자. 우선 ForwardA는 R&lt;sub&gt;s&lt;/sub&gt;, ForwardA는 R&lt;sub&gt;t&lt;/sub&gt;에 대한 MUX를 제어한다. 두 신호 공통으로 00은 Read Register에서 읽어온 값을 그대로 선택한다는 뜻이고, 10은 ALU 결과를 포워딩, 01은 메모리에서 읽은 값을 포워딩한다는 의미이다. 값을 결정하는 기준은 앞에서 얘기한 Data Hazard를 판단하는 네 가지 경우와 RegWrite 상태, R&lt;sub&gt;d&lt;/sub&gt;가 &lt;code&gt;$0&lt;/code&gt;이 아닌지이다.&lt;/p&gt;
&lt;p&gt;  정리해보면 아래와 같다.&lt;/p&gt;
&lt;p&gt;1.EX hazard:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if (EX/MEM.RegWrite
  and (EX/MEM.RegisterRd ≠ 0)
  and (EX/MEM.RegisterRd = ID/EX.RegisterRs)) ForwardA = 10 

if (EX/MEM.RegWrite
  and (EX/MEM.RegisterRd ≠ 0)
  and (EX/MEM.RegisterRd = ID/EX.RegisterRt)) ForwardB = 10 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.MEM hazard:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   if (MEM/WB.RegWrite
     and (MEM/WB.RegisterRd ≠  0)
     and (MEM/WB.RegisterRd = ID/EX.RegisterRs)) ForwardA = 01
   if (MEM/WB.RegWrite
     and (MEM/WB.RegisterRd ≠  0)
     and (MEM/WB.RegisterRd = ID/EX.RegisterRt)) ForwardB = 01
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="doubledatahazard"&gt;Double Data Hazard&lt;/h2&gt;
&lt;p&gt;  아래 코드를 보자.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-assembly"&gt;add $1, $1, $2
add $1, $1, $3
add $1, $1, $4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;위 코드의 경우, 세 번째 &lt;code&gt;add&lt;/code&gt;에서 두 종류의 Hazard가 동시에 발생하는데 세 번째 &lt;code&gt;add&lt;/code&gt;가 EX 단계에 들어가는 상황을 생각해보자. 우선, 세 번째 &lt;code&gt;add&lt;/code&gt;의 R&lt;sub&gt;s&lt;/sub&gt;(ID/EX)가 두 번째 &lt;code&gt;add&lt;/code&gt;의 R&lt;sub&gt;d&lt;/sub&gt;(EX/MEM)와 같기 때문에 EX Hazard가 발생했다. 또 세 번째 &lt;code&gt;add&lt;/code&gt;의 R&lt;sub&gt;s&lt;/sub&gt;는 첫 번째 &lt;code&gt;add&lt;/code&gt;의 R&lt;sub&gt;d&lt;/sub&gt;(MEM/WB)와도 같아서 MEM Hazard도 동시에 발생한다.&lt;/p&gt;
&lt;p&gt;  위 예시처럼 EX Hazard와 MEM Hazard가 동시에 발생했을 때 EX/MEM의 R&lt;sub&gt;d&lt;/sub&gt;와 MEM/WB의 R&lt;sub&gt;d&lt;/sub&gt; 중에서 더 최근 값인 두 번째 &lt;code&gt;add&lt;/code&gt;의 R&lt;sub&gt;d&lt;/sub&gt;, 즉 EX/MEM의 값을 포워딩해야 한다. 다시 말해 MEM Hazard일 때 포워딩하기 위한 조건에 EX Hazard가 아니어야 한다는 조건이 더 추가되어야 한다. 이 조건을 추가한 MEM Hazard 조건은 아래와 같다.&lt;/p&gt;
&lt;p&gt;2.MEM hazard:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   if (MEM/WB.RegWrite
     and (MEM/WB.RegisterRd ≠0)
     and not(EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠0)
           and (EX/MEM.RegisterRd ≠  ID/EX.RegisterRs))
     and (MEM/WB.RegisterRd = ID/EX.RegisterRs)) ForwardA = 01
   if (MEM/WB.RegWrite
     and (MEM/WB.RegisterRd ≠0)
     and not(EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠0)
           and (EX/MEM.RegisterRd ≠  ID/EX.RegisterRs))
     and (MEM/WB.RegisterRd = ID/EX.RegisterRt)) ForwardB = 01
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="loadusedatahazard"&gt;Load-Use Data Hazard&lt;/h2&gt;
&lt;p&gt;  Load-Use Data Hazard에 대해서는 &amp;quot;4.5 An Overview of Pipelining(2)&amp;quot; 포스팅에서 설명했으므로 자세한 설명은 일단 생략하겠다. 이러한 경우에는 무조건 Stall(대기 상태)이 한 사이클 발생하는데, 회로에서 구현하기 위해서는 Load-Use Data Hazard를 감지한 다음에 해당 instruction을 일부러 한 사이클 늦게 실행해야 한다. 다음은 파이프라인에서 Stall을 만드는 방법이다.&lt;/p&gt;
&lt;p&gt;Load-Use Data Hazard는 아래 조건과 같이 감지할 수 있다.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; if (ID/EX.MemRead and
   ((ID/EX.RegisterRt = IF/ID.RegisterRs) or
     (ID/EX.RegisterRt = IF/ID.RegisterRt)))
       stall the pipeline
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="howtostallthepipeline"&gt;How to Stall the Pipeline&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;ID / EX 레지스터의 Control 값을 전부 0으로 만든다.
&lt;ul&gt;
&lt;li&gt;Control이 전부 0이면 EX, MEM, WB에서 아무 연산도 하지 않는다.(&lt;em&gt;&lt;strong&gt;nop&lt;/strong&gt;&lt;/em&gt; : no-operation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PC와 IF/ID 레지스터를 업데이트 하지 않는다.
&lt;ul&gt;
&lt;li&gt;IF 단계와 ID 단계를 한 사이클 반복한다.&lt;/li&gt;
&lt;li&gt;그러면 같은 동작을 반복하므로 IF, ID도 사실상 한 사이클 동안 &lt;strong&gt;nop&lt;/strong&gt; 상태라고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; 아래는 Load-Use Data Hazard를 위와 같은 방식으로 처리한 것을 multi-cycle pipeline diagram으로 나타낸 이미지 &lt;em&gt;(강의 교재 figure 4.59)&lt;/em&gt; 이다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-7-3.png" alt="4.7 Data Hazards: Forwarding versus Stalling"&gt;&lt;/p&gt;
&lt;p&gt;  여기서는 nop 이후에 &lt;code&gt;and&lt;/code&gt;의 IF를 한 번 더 실행하는 것처럼 표현되었지만, 실제로는 두 번째 사이클에서 가져온 instruction을 한 사이클 대기한 뒤 EX 단계로 넘어가는 거지만 그냥 넘어가자.&lt;/p&gt;
&lt;p&gt; Load-Use Data Hazard를 감지하는 Hazard Detection Unit을 추가한 최종 회로의 이미지 &lt;em&gt;(강의 교재 figure 4.60)&lt;/em&gt; 를 끝으로 포스팅을 마칩니다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-7-4.png" alt="4.7 Data Hazards: Forwarding versus Stalling"&gt;&lt;/p&gt;
&lt;!--Image Credit(Freepik) --&gt;
&lt;a style="font-family:sans-serif; font-size: 15px; color: #26a8ed;
    text-decoration: none; box-shadow: none;" href="https://www.freepik.com/free-photos-vectors/background"&gt;Header Image created by Freepik&lt;/a&gt;</content:encoded></item><item><title>4.6 Pipelined Datapath and Control</title><description>&lt;h1 id="pipelineddatapathandcontrol"&gt;Pipelined Datapath and Control&lt;/h1&gt;
&lt;p&gt;  여기서는 4.4장에서 완성했던 기존의 Single-Cycle Datapath를 Pipelined Datapath로 바꾸는 과정을 살펴볼 예정이다. 우선 회로를 아래 다섯 단계로 나누어 표시해보자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IF&lt;/strong&gt;(Instruction Fetch)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;(Instruction Decode, Register File Read)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EX&lt;/strong&gt;(Execute, Address Calculation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MEM&lt;/strong&gt;(Memory Access)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WB&lt;/strong&gt;(Write Back)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-1.png" alt="computer-architecture-4-6-1"&gt;&lt;/p&gt;
&lt;p&gt;  위 이미지 &lt;em&gt;(강의 교재 figure 4.33)&lt;/em&gt;&lt;/p&gt;</description><link>http://identity16.github.io/4-6-pipelined-datapath-and-control/</link><guid isPermaLink="false">5c35fa252d91b6c5113154be</guid><category>computer-architecture</category><dc:creator>Wonjun Shin</dc:creator><pubDate>Wed, 12 Dec 2018 08:52:00 GMT</pubDate><media:content url="http://identity16.github.io/content/images/2019/01/water-pipe-51758_1280.jpg" medium="image"/><content:encoded>&lt;h1 id="pipelineddatapathandcontrol"&gt;Pipelined Datapath and Control&lt;/h1&gt;
&lt;img src="http://identity16.github.io/content/images/2019/01/water-pipe-51758_1280.jpg" alt="4.6 Pipelined Datapath and Control"&gt;&lt;p&gt;  여기서는 4.4장에서 완성했던 기존의 Single-Cycle Datapath를 Pipelined Datapath로 바꾸는 과정을 살펴볼 예정이다. 우선 회로를 아래 다섯 단계로 나누어 표시해보자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IF&lt;/strong&gt;(Instruction Fetch)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;(Instruction Decode, Register File Read)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EX&lt;/strong&gt;(Execute, Address Calculation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MEM&lt;/strong&gt;(Memory Access)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WB&lt;/strong&gt;(Write Back)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-1.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;p&gt;  위 이미지 &lt;em&gt;(강의 교재 figure 4.33)&lt;/em&gt; 과 같이 나눌 수 있는데, 여기서 파란색 선은 다른 단계로 데이터가 전달되는 상황임을 표시한 것이다. 위쪽 파란 선은 MEM에서 IF로, 아래쪽 파란 선은 WB에서 ID로 데이터가 전달되는 부분이다. WB -&amp;gt; ID의 Right-to-Left 상황은 Data Hazard와 Control Hazard를 유발한다. 이것을 해결하는 방법은 둘째 치고, 각 단계에서 각기 다른 Instruction을 수행해야 하므로 단계마다 다른 데이터를 이용해 연산을 해야한다. 따라서 단계의 사이사이에 사용할 데이터를 저장하는 레지스터를 추가한다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-2.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;p&gt;  추가된 레지스터의 이름은 양쪽의 두 단계의 이름을 합친 IF/ID, ID/EX, EX/MEM, MEM/WB이다. 이 레지스터들도 State Element이므로 각 클럭마다 상태가 업데이트 되어야 한다. 따라서 위 이미지 &lt;em&gt;(강의 교재 figure 4.35)&lt;/em&gt; 의 회로에는 이 레지스터들과 PC에 Clock 입력이 있어야 한다.&lt;/p&gt;
&lt;h2 id="representingpipelineoperation"&gt;Representing Pipeline Operation&lt;/h2&gt;
&lt;p&gt;  파이프라인의 상태를 표현할 때 우리는 다음 두 가지 방식을 사용할 것이다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Single-clock-cycle&amp;quot; pipeline diagram : 특정 사이클에서 사용되는 부분을 표시하고 단계 별로 어떤 Instruction을 실행하고 있는지 표시한 그림이다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Multi-clock-cycle&amp;quot; pipeline diagram : 아래 이미지 &lt;em&gt;(강의 교재 figure 4.34)&lt;/em&gt; 참고&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-3.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;p&gt;  이제 Load에 대한 Single-clock-cycle 다이어그램을 살펴보도록 하자. 전부 살펴보지는 않고 IF 단계와 WB 단계만 보고 넘어갈 예정이다.&lt;/p&gt;
&lt;h3 id="ifforload"&gt;IF for Load&lt;/h3&gt;
&lt;p&gt;  제일 첫 번째 단계인 IF 단계의 다이어그램이다. 아래 이미지 &lt;em&gt;(강의 교재 figure 4.36)&lt;/em&gt; 를 보고 Single-clock-cycle 다이어그램이 어떻게 생겼는지 살펴보길 바란다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-4.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;h3 id="wbforload"&gt;WB for Load&lt;/h3&gt;
&lt;p&gt;  이번에는 WB 단계인데, 우선 이미지 &lt;em&gt;(강의 교재 figure 4.38)&lt;/em&gt; 부터 보자.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-5.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;p&gt;  다른 단계를 건너뛰고 바로 WB 단계를 소개하는 이유는 위 회로에서 오류가 하나 있기 때문이다. 그것은 바로 ID 단계의 Write Data로 값을 넘기는 부분인데, 이대로 회로를 사용한다면 넘어간 데이터가 쓰여질 레지스터는 ID 단계에서 실행 중인 Instruction의 Write Register이어서 엉뚱한 곳에 쓰이게 된다. 이를 해결하기 위해서는 WB 단계의 Instruction에서 사용하는 Write Register 넘버를 같이 넘겨줘야 한다. 그렇게 구성한 회로는 아래 이미지 &lt;em&gt;(강의 교재 figur 4.41)&lt;/em&gt; 에 있다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-6.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;h2 id="pipelinedcontrol"&gt;Pipelined Control&lt;/h2&gt;
&lt;p&gt;  지금까지 작성한 회로는 동작 상으로는 문제가 없겠지만 뭔가 부족한 점이 하나 있다. 바로 Control의 부재이다. Control 신호도 앞선 데이터들과 마찬가지로 Instruction마다 다르기 때문에 단계가 넘어갈 때마다 레지스터에 저장해 주어야 한다. 그러나 모든 신호를 매번 다 저장하면 공간낭비이므로 단계별로 사용하는 신호를 나눈 다음, 더이상 필요없는 신호들은 다음 단계로 갈 때 저장하지 않도록 한다. 아래 표 &lt;em&gt;(강의 교재 figure 4.49)&lt;/em&gt; 는 단계 별로 Control 신호를 나눈 것이다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-7.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;p&gt;그리고 Control 신호를 저장할 레지스터를 덧붙인 상태의 회로는 아래 &lt;em&gt;(강의 교재 figure 4.51)&lt;/em&gt; 와 같이 생겼다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://identity16.github.io/content/images/2019/01/computer-architecture-4-6-8.png" alt="4.6 Pipelined Datapath and Control"&gt;&lt;/p&gt;
&lt;p&gt;완성!은 아직 아니고 Hazard를 해결하는 걸 조금 더 해야한다. 그건 다음 포스팅에서 다룰 예정이다.&lt;/p&gt;
</content:encoded></item></channel></rss>